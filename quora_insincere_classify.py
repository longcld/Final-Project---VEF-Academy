# -*- coding: utf-8 -*-
"""Quora_Insincere_Classify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qHPsi2vOw_WiWikK0u1znbCUpCwjTFVa

#Final Project

###Import package
"""

import math
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import nltk
import string
from wordcloud import WordCloud
from nltk.corpus import stopwords
nltk.download('stopwords')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn import metrics

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, Layer
from tensorflow.keras.initializers import Constant
from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Bidirectional, Input, Flatten, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

"""###Load data"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip
!unzip glove*.zip
!unzip wiki-news-300d-1M.vec.zip
!wget https://dl.dropboxusercontent.com/s/mll1vn14zqcb1os/train.csv
!wget https://dl.dropboxusercontent.com/s/607wpoldwv10xn3/test.csv

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

"""##Exploratory Data Analysis

###Check distribution of target
"""

target = train['target']
values_count = list(target.value_counts())
tar = ('Sincere', 'Insincere')

fig, ax = plt.subplots(figsize = (6, 8))
ax.bar([0, 1], values_count, align = 'center', color = 'pink')
ax.text(-0.1, values_count[0] + 10000, s = str(round(values_count[0] * 100 / len(target), 2)) + '%', fontsize = 12)
ax.text(0.9, values_count[1] + 10000, s = str(round(values_count[1] * 100 / len(target), 2)) + '%', fontsize = 12)
ax.set_xticks([0, 1])
ax.set_xticklabels(tar, fontsize = 12)
ax.set_title("Distribution of target values", fontsize = 14)
plt.savefig("/content/Distribution_of_target.png")
plt.show()

train.isnull().sum()

"""###Check length of questions"""

print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x.split())))))
print('Average word length of questions in test is {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x.split())))))
print('Max word length of questions in train is {0:.0f}.'.format(np.max(train['question_text'].apply(lambda x: len(x.split(" "))))))
print('Max word length of questions in test is {0:.0f}.'.format(np.max(test['question_text'].apply(lambda x: len(x.split(" "))))))
print('Average character length of questions in train is {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x)))))
print('Average character length of questions in test is {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x)))))
print('Average word length of sincere questions in train is {0:.0f}.'.format(np.mean(train['question_text'][train['target'] == 0].apply(lambda x: len(x.split())))))
print('Average word length of insincere questions in train is {0:.0f}.'.format(np.mean(train['question_text'][train['target'] == 1].apply(lambda x: len(x.split())))))
print('Max word length of sincere questions in train is {0:.0f}.'.format(np.max(train['question_text'][train['target'] == 0].apply(lambda x: len(x.split())))))
print('Max word length of insincere questions in train is {0:.0f}.'.format(np.max(test['question_text'][train['target'] == 1].apply(lambda x: len(x.split())))))

"""###Word count plot"""

punctuations = string.punctuation
stop_word = stopwords.words('english')
def stopwords_punctuation(sentence):
    s = ''
    for punc in punctuations:
        sentence = sentence.replace(punc, '')
    for word in sentence.lower().split(" "):
        if word not in stop_word:
            s += word + " "
    return s[:len(s) - 1]

def remove_stopwords_punctuation(df):
    clean = []
    for sentence in tqdm(df['question_text'].values):
        clean.append(stopwords_punctuation(sentence))
    return clean

sincere = train[train['target'] == 0]
insincere = train[train['target'] == 1]

clean_sincere = remove_stopwords_punctuation(sincere)
clean_insincere = remove_stopwords_punctuation(insincere)
clean_data = remove_stopwords_punctuation(train)

wc_sincere = WordCloud().generate(' '.join(clean_sincere).lower())
wc_insincere = WordCloud().generate(' '.join(clean_insincere).lower())

fig, ax = plt.subplots(1, 2, figsize = (20, 6))

ax[0].imshow(wc_sincere)
ax[0].set_title("Top used word in sincere question")
ax[0].axis('off')

ax[1].imshow(wc_insincere)
ax[1].set_title("Top used word in insincere question")
ax[1].axis('off')
plt.show()

"""##Building model

## Logistic Regression
"""

X_train, X_test, Y_train, Y_test = train_test_split(train['question_text'], train['target'], test_size = 0.2, random_state = 5)

tfv = TfidfVectorizer(min_df=3, max_features=None, 
                      strip_accents='unicode', analyzer='word',
                      ngram_range=(1, 3))
tfv.fit(clean_data)
x_train = tfv.transform(X_train.values)
x_test = tfv.transform(X_test)

LR = LogisticRegressionCV(penalty = 'l2', solver='lbfgs', max_iter=20, random_state=1)
LR.fit(x_train, Y_train)

f1 = metrics.f1_score(y_true = Y_test, y_pred = LR.predict(x_test))
acc = metrics.accuracy_score(y_true = Y_test, y_pred = LR.predict(x_test))

print("F1-score on test set: {}".format(f1))
print("Accuracy score on test set: {}".format(acc))

"""##BiLSTM

###Preprocessing question
"""

max_features = 100000
max_len = 70
embedding_dim = 300

def preprocessing(data, get_word_index = False):
    data = list(data['question_text'].values)
    tokenizer = Tokenizer(num_words=max_features, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', split=' ')
    tokenizer.fit_on_texts(data)
    sequence = tokenizer.texts_to_sequences(data)
    if get_word_index:
        return tokenizer.word_index
    else:
        return pad_sequences(sequence, maxlen=max_len)

"""###Embedding"""

def get_word_index(data):
    data = list(data['question_text'].values)
    tokenizer = Tokenizer(num_words=max_features, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', split=' ')
    tokenizer.fit_on_texts(data)
    sequence = tokenizer.texts_to_sequences(data)
    return pad_sequences(sequence, maxlen=max_len), tokenizer.word_index

def load_glove(word_index):
    embeddings_index = {}
    with open('glove.6B.300d.txt', encoding='utf8') as f:
        for line in tqdm(f):
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:],dtype='float32')
            embeddings_index[word] = coefs

    all_embs = np.stack(embeddings_index.values())
    emb_mean,emb_std = all_embs.mean(), all_embs.std()
    embed_size = all_embs.shape[1]

    nb_words = min(max_features, len(word_index))
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
    for word, i in word_index.items():
        if i >= max_features: continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None: embedding_matrix[i] = embedding_vector
            
    return embedding_matrix

def load_fast_text(word_index):
    embeddings_index = {}
    with open('wiki-news-300d-1M.vec', encoding='utf8') as f:
        for line in tqdm(f):
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:],dtype='float32')
            embeddings_index[word] = coefs

    all_embs = np.stack(embeddings_index.values())
    emb_mean,emb_std = all_embs.mean(), all_embs.std()
    embed_size = len(embeddings_index)

    nb_words = min(max_features, len(word_index))
    embedding_matrix = np.random.random((nb_words, embed_size))
    for word, i in word_index.items():
        if i >= max_features: continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None: embedding_matrix[i] = embedding_vector
            
    return embedding_matrix

pad, word_index = get_word_index(train)
embedding_matrix = load_glove(word_index)

X_train, X_test, Y_train, Y_test = train_test_split(pad, train['target'], test_size = 0.2, random_state = 5)
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 15)

"""###Training"""

inp = Input(shape=(max_len,))
outp = Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False)(inp)
outp = Bidirectional(CuDNNLSTM(128, return_sequences=True))(outp)
outp = Bidirectional(CuDNNLSTM(64, return_sequences=True, ))(outp)
outp = Bidirectional(CuDNNLSTM(32, return_sequences=True))(outp)
outp = Dropout(0.2)(outp)
outp = Flatten()(outp)
outp = Dense(512, activation="relu")(outp)
outp = Dropout(0.2)(outp)
outp = Dense(1, activation="sigmoid")(outp)
model = Model(inputs=inp, outputs=outp)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

for i in range(10):
    model.fit(X_train, Y_train, 
              epochs = 1, 
              batch_size=512, 
              validation_data=(X_val, Y_val), 
              shuffle=True,
              callbacks=[ModelCheckpoint('/result.h5',save_best_only=True),
                         EarlyStopping()])
    pred_val_y = model.predict([X_val], batch_size=1024, verbose=0)
    
    best_thresh = 0.5
    best_score = 0.0
    for thresh in np.arange(0.1, 0.501, 0.01):
        thresh = np.round(thresh, 2)
        score = metrics.f1_score(Y_val, (pred_val_y > thresh).astype(int))
        if score > best_score:
            best_thresh = thresh
            best_score = score

    print("Val F1 Score: {:.4f}, Best Threshold: {:.4f}".format(best_score, best_thresh))

pred_val_y = model.predict([X_test], batch_size=1024, verbose=0)

best_thresh = 0.5
best_score = 0.0
for thresh in np.arange(0.1, 0.501, 0.01):
    thresh = np.round(thresh, 2)
    score = metrics.f1_score(Y_test, (pred_val_y > thresh).astype(int))
    if score > best_score:
        best_thresh = thresh
        best_score = score

print("Val Accuracy Score: {:.4f}, Best Threshold: {:.4f}".format(best_score, best_thresh))

